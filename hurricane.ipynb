{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLyQEBUEQIP3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "22762103-e7a6-4a49-ffb9-cb9b9c63b577"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 10000 files belonging to 2 classes.\n",
            "Found 2000 files belonging to 2 classes.\n",
            "Epoch 1/15\n",
            "157/157 [==============================] - 9s 51ms/step - loss: 0.5248 - accuracy: 0.7325 - val_loss: 0.3306 - val_accuracy: 0.8565\n",
            "Epoch 2/15\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.2787 - accuracy: 0.8830 - val_loss: 0.2465 - val_accuracy: 0.8850\n",
            "Epoch 3/15\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.1762 - accuracy: 0.9291 - val_loss: 0.1703 - val_accuracy: 0.9260\n",
            "Epoch 4/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.1275 - accuracy: 0.9481 - val_loss: 0.1376 - val_accuracy: 0.9460\n",
            "Epoch 5/15\n",
            "157/157 [==============================] - 7s 46ms/step - loss: 0.1058 - accuracy: 0.9573 - val_loss: 0.1012 - val_accuracy: 0.9605\n",
            "Epoch 6/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.1015 - accuracy: 0.9599 - val_loss: 0.1114 - val_accuracy: 0.9595\n",
            "Epoch 7/15\n",
            "157/157 [==============================] - 8s 49ms/step - loss: 0.0720 - accuracy: 0.9714 - val_loss: 0.0953 - val_accuracy: 0.9650\n",
            "Epoch 8/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0684 - accuracy: 0.9749 - val_loss: 0.1026 - val_accuracy: 0.9650\n",
            "Epoch 9/15\n",
            "157/157 [==============================] - 8s 48ms/step - loss: 0.0516 - accuracy: 0.9812 - val_loss: 0.1051 - val_accuracy: 0.9635\n",
            "Epoch 10/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0576 - accuracy: 0.9791 - val_loss: 0.1218 - val_accuracy: 0.9545\n",
            "Epoch 11/15\n",
            "157/157 [==============================] - 7s 45ms/step - loss: 0.0384 - accuracy: 0.9866 - val_loss: 0.0996 - val_accuracy: 0.9675\n",
            "Epoch 12/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0278 - accuracy: 0.9911 - val_loss: 0.1290 - val_accuracy: 0.9660\n",
            "Epoch 13/15\n",
            "157/157 [==============================] - 6s 36ms/step - loss: 0.0198 - accuracy: 0.9938 - val_loss: 0.2223 - val_accuracy: 0.9590\n",
            "Epoch 14/15\n",
            "157/157 [==============================] - 6s 40ms/step - loss: 0.0251 - accuracy: 0.9913 - val_loss: 0.1519 - val_accuracy: 0.9515\n",
            "Epoch 15/15\n",
            "157/157 [==============================] - 6s 37ms/step - loss: 0.0197 - accuracy: 0.9937 - val_loss: 0.1191 - val_accuracy: 0.9680\n"
          ]
        }
      ],
      "source": [
        "# ======================================================================\n",
        "# There are 5 questions in this exam with increasing difficulty from 1-5.\n",
        "# Please note that the weight of the grade for the question is relative to its\n",
        "# difficulty. So your Category 1 question will score significantly less than\n",
        "# your Category 5 question.\n",
        "#\n",
        "# WARNING: Do not use lambda layers in your model, they are not supported\n",
        "# on the grading infrastructure. You do not need them to solve the question.\n",
        "#\n",
        "# You must use the Submit and Test button to submit your model\n",
        "# at least once in this category before you finally submit your exam,\n",
        "# otherwise you will score zero for this category.\n",
        "# ======================================================================\n",
        "#\n",
        "# COMPUTER VISION WITH CNNs\n",
        "#\n",
        "# Create and train a classifier to classify images between two classes\n",
        "# (damage and no_damage) using the satellite-images-of-hurricane-damage dataset.\n",
        "# ======================================================================\n",
        "#\n",
        "# ABOUT THE DATASET\n",
        "#\n",
        "# Original Source:\n",
        "# https://ieee-dataport.org/open-access/detecting-damaged-buildings-post-hurricane-satellite-imagery-based-customized\n",
        "# The dataset consists of satellite images from Texas after Hurricane Harvey\n",
        "# divided into two groups (damage and no_damage).\n",
        "# ==============================================================================\n",
        "#\n",
        "# INSTRUCTIONS\n",
        "#\n",
        "# We have already divided the data for training and validation.\n",
        "#\n",
        "# Complete the code in following functions:\n",
        "# 1. preprocess()\n",
        "# 2. solution_model()\n",
        "#\n",
        "# Your code will fail to be graded if the following criteria are not met:\n",
        "# 1. The input shape of your model must be (128,128,3), because the testing\n",
        "#    infrastructure expects inputs according to this specification. You must\n",
        "#    resize all the images in the dataset to this size while pre-processing\n",
        "#    the dataset.\n",
        "# 2. The last layer of your model must be a Dense layer with 1 neuron\n",
        "#    activated by sigmoid since this dataset has 2 classes.\n",
        "#\n",
        "# HINT: Your neural network must have a validation accuracy of approximately\n",
        "# 0.95 or above on the normalized validation dataset for top marks.\n",
        "\n",
        "import urllib\n",
        "import zipfile\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "# This function downloads and extracts the dataset to the directory that\n",
        "# contains this file.\n",
        "# DO NOT CHANGE THIS CODE\n",
        "# (unless you need to change https to http)\n",
        "def download_and_extract_data():\n",
        "    url = 'https://storage.googleapis.com/download.tensorflow.org/data/certificate/satellitehurricaneimages.zip'\n",
        "    urllib.request.urlretrieve(url, 'satellitehurricaneimages.zip')\n",
        "    with zipfile.ZipFile('satellitehurricaneimages.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall()\n",
        "\n",
        "# This function normalizes the images.\n",
        "# COMPLETE THE CODE IN THIS FUNCTION\n",
        "def preprocess(image, label):\n",
        "    # NORMALIZE YOUR IMAGES HERE (HINT: Rescale by 1/.255)\n",
        "    # image = image/255.0\n",
        "    # label = label/255.0\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "\n",
        "    return image, label\n",
        "\n",
        "\n",
        "# This function loads the data, normalizes and resizes the images, splits it into\n",
        "# train and validation sets, defines the model, compiles it and finally\n",
        "# trains the model. The trained model is returned from this function.\n",
        "\n",
        "# COMPLETE THE CODE IN THIS FUNCTION.\n",
        "def solution_model():\n",
        "    # Downloads and extracts the dataset to the directory that\n",
        "    # contains this file.\n",
        "    download_and_extract_data()\n",
        "\n",
        "    IMG_SIZE = (128,128)\n",
        "    BATCH_SIZE = 64\n",
        "\n",
        "    # The following code reads the training and validation data from their\n",
        "    # respective directories, resizes them into the specified image size\n",
        "    # and splits them into batches. You must fill in the image_size\n",
        "    # argument for both training and validation data.\n",
        "    # HINT: Image size is a tuple\n",
        "    train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='train/',\n",
        "        image_size= IMG_SIZE\n",
        "        , batch_size=BATCH_SIZE)\n",
        "\n",
        "    val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "        directory='validation/',\n",
        "        image_size=  IMG_SIZE\n",
        "        , batch_size=BATCH_SIZE)\n",
        "    # Normalizes train and validation datasets using the\n",
        "    # preprocess() function.\n",
        "    # Also makes other calls, as evident from the code, to prepare them for\n",
        "    # training.\n",
        "    # Do not batch or resize the images in the dataset here since it's already\n",
        "    # been done previously.\n",
        "    train_ds = train_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE).prefetch(\n",
        "        tf.data.experimental.AUTOTUNE)\n",
        "    val_ds = val_ds.map(\n",
        "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # Code to define the model\n",
        "    model = tf.keras.models.Sequential([\n",
        "        # ADD LAYERS OF THE MODEL HERE\n",
        "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D((2, 2)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "        # If you don't adhere to the instructions in the following comments,\n",
        "        # tests will fail to grade your model:\n",
        "        # The input layer of your model must have an input shape of\n",
        "        # (128,128,3).\n",
        "        # Make sure your last layer has 1 neuron activated by sigmoid.\n",
        "        tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)\n",
        "    ])\n",
        "\n",
        "    # Code to compile and train the model\n",
        "    model.compile( optimizer='adam',loss = 'binary_crossentropy',metrics = ['accuracy']\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "\n",
        "    model.fit(train_ds,validation_data = val_ds,epochs  = 15,verbose = 1\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# Note that you'll need to save your model as a .h5 like this.\n",
        "# When you press the Submit and Test button, your saved .h5 model will\n",
        "# be sent to the testing infrastructure for scoring\n",
        "# and the score will be returned to you.\n",
        "if __name__ == '__main__':\n",
        "    model = solution_model()\n",
        "    #"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('mymodel_h.h5')"
      ],
      "metadata": {
        "id": "wjoUXYCobPb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZwSvDe1Y7jR",
        "outputId": "13a321ea-bc70-4109-d750-1a3cf419f9ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.5.0"
      ],
      "metadata": {
        "id": "o1s81WfMem5S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1441d3a6-0b29-42ec-cbcf-3ca03851518b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow==2.5.0 in /usr/local/lib/python3.9/dist-packages (2.5.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.40.0)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (2.11.2)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.2.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.6.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.4.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.19.6)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (0.15.0)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.19.5)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.7.4.3)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.12)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (2.5.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.12.1)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.34.1)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow==2.5.0) (3.3.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.2.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (67.6.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard~=2.5->tensorflow==2.5.0) (2.16.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (5.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (6.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow==2.5.0) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.5->tensorflow==2.5.0) (2.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.5->tensorflow==2.5.0) (3.15.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.5->tensorflow==2.5.0) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow==2.5.0) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYJElJy-Vg8b",
        "outputId": "521283fb-6603-494c-f664-16ebc1e42425"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sfPiNLWTVyXg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}